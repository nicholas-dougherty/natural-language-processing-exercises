from requests import get
from bs4 import BeautifulSoup
import pandas as pd
from time import strftime
import os
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
#-----------------------1. Accessing Codeup's Blog----------------------------------
                            
def get_blog_links():
    """
    Uses beautiful soup to request and record information from Codeup's blog, 
    using the necessary headers that grant access. 
    The use of .attrs (line 22) initializes a dictionary containing the hyperlinks. 
    """
    # Make request to gain access to codeup's HTML for their blog site.
    response = get("https://codeup.com/blog/", headers={"user-agent": "Codeup Data Science"})
    # Create Beautiful Soup to get parse tree of the pages parsed in the request.
    soup = BeautifulSoup(response.text)
    # Initialize a dictionary which fills with the links obtained through soup.select.
    links = [link.attrs["href"] for link in soup.select(".more-link")]
    #[link['href'] for link in soup.select('.more-link')]

    return links
#-----------------------------------------------------------------------------------
def parse_blog(url):
    """
    Copy and paste the specific blog you wish to parse and put it in quotes
    as a string for the argument. This UDF then proceeds with extraction:
    the title, publication date, and content therein are returned as the 
    extracted elements; all as a dictionary.
    """  
    # Make request to gain access to codeup's HTML for their blog site.
    response = get(url, headers={"user-agent": "Codeup Data Science"})
    # Create Beautiful Soup to get parse tree of the pages parsed in the request.
    soup = BeautifulSoup(response.text)
    # Use select_one functions from Beautiful Soup to access dictionary entries.
    return {
        "title": soup.select_one(".entry-title").text,
        "publication_date": soup.select_one(".published").text,
        "content": soup.select_one(".entry-content").text.strip(),
    }

#-----------------------------------------------------------------------------------
def get_blog_df():
    """
    A master function that combines the execution of the two previous UDFs and
    subsequently returns a dataframe generated by a for-loop.
    """
    links = get_blog_links()
    df = pd.DataFrame([parse_blog(link) for link in links])
    return df

#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
#-----------------------2. Accessing a News Site------------------------------------

def get_news_articles_for_one_category (category, url):
    """
    Using the requests.get and bs4.beautiful soup libraries' strategies, this UDF
    parses for instances of 'div' elements, separated as independent articles.
    Runs through a for-loop, finding classes by their respective itemized properties.
    Garners this information into a data dictionary, and appends to the output array
    for each instance in the loop. Returns the dictionary for perusal.
    """
    # generate an empty list
    output = []
    # html request through the url given in the argument
    response = get(url)
    # creates soup to extrapolate content in the proceeding lines.
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # find every instance of div,the class_ differentiates among unique articles
    divs = soup.find_all('div', class_='news-card z-depth-1')
    
    # within each divs subgroup:
    for article in divs:
        # search in span via the property headline
        title = article.find('span', itemprop='headline')
        # gather content by each div, per the properties of the article's 'body'
        content = article.find('div', itemprop='articleBody')
        # implement these elements into a data dictionary---
        # include the category given in the UDF arguments
        data = {'title': title.text, 'content': content.text, 'category': category}
        # append this to the output array
        output.append(data)
        # show me the money (the filled array)
    return output

def get_news_articles(input):
    """
    Input is primarily dictionary, set up as the category followed by the url that contains it.
    The final output starts as an empty array/list. The dictionary's key and value is
    scanned per each item; using the UDF above, the articles per category are retrieved
    and added segmentally to the initial empty list. Once this for-loop has run the gamut,
    the content is turned into a dictionary and returned to the user.
    """
    # an empty box to fill with ideas. 
    final_output = []
    # loop through the dictionary and call the previous UDF for each category/site pair
    for key, value in input.items():
        # a list within the loop for collecting the contents per category
        temp_list = get_news_articles_for_one_category(key, value)
        # for each article fully examined, add it to the initial list. 
        final_output += temp_list
     # Create a dataframe based on the final output. 
    df = pd.DataFrame(final_output)
    # gimme that df right meow. 
    return df

#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------
#-----------------------Missing_no: Alternate methods-------------------------------
# Different solutions to the same two problems. 
# Classroom content

def get_blog_article_urls():
    """
    Similar to the first instance. Takes the necessary headers to 
    authorize scraping. Generates a response that Soup extracts 
    per 'href', selecting the additional link, and thus providing
    us with all the urls on a given page.
    """
    # Essential to get a 200 status code. 
    headers = {'user-agent': 'Innis Codeup Data Science'}
    # headers must be included. Requests.get retrieves the content. 
    response = get('https://codeup.com/blog/', headers=headers)
    # The soupy star from previous UDFs. Bowling for itself. 
    soup = BeautifulSoup(response.text, features='lxml')
    # the key to gaining each url for/in; set as a list variable.
    urls = [a.attrs['href'] for a in soup.select('a.more-link')]
    # returns a list of urls for use in the following. 
    return urls


def parse_blog_article(soup):
    """
    Very straightforward soup for the parsing. 
    The key and values are respective to their methodologies.
    Returns a dictionary with a cleanly formatted set of keys:
    title, publication date, content (i.d. paragraph bodies)
    """
    return {
        'title': soup.select_one('h1.entry-title').text,
        'published': soup.select_one('.published').text,
        'content': soup.select_one('.entry-content').text.strip(),
    }


def get_blog_articles(use_cache=True):
    if os.path.exists('codeup_blog_articles.json') and use_cache:
        return pd.read_json('codeup_blog_articles.json')
    
    headers = {'user-agent': 'Innis Codeup Data Science'}
    urls = get_blog_article_urls()
    articles = []

    for url in urls:
        print(f'fetching {url}')
        response = get(url, headers=headers)
        soup = BeautifulSoup(response.text, features='lxml')
        articles.append(parse_blog_article(soup))

    df = pd.DataFrame(articles)
    df.to_json('codeup_blog_articles.json', orient='records')
    return df

def parse_news_card(card, category):
    ''' 
    Starts with an empty dictionary, selects an anchor tag of the class 'clickable'
    and strips it of its excess, delivered in text form to the dictionary's title label.
    The news card content is selected and the output is fed into the dictionary via 
    the first instance of div. Additionally, the element of the class author has its
    details included, as does the time via a machine-readable time-stamp via the content
    attribute via the .attrs on a beautiful soup object date. 
    Once the category, title, author, and publication date are all garnered, the function
    returns the dictionary. 
    '''
    output = {}

    output['category'] = category
    output['title'] = card.select_one('a.clickable').text.strip()

    card_content = card.select_one('.news-card-content')
    output['content'] = card_content.select_one('div').text

    author_and_time = card_content.select_one('.news-card-author-time')
    output['author'] = author_and_time.select_one('.author').text
    output['published'] = author_and_time.select_one('.time').attrs['content']

    return output


def parse_news_category(category):
    '''
    This UDF implements parse_news_card, but a particular category 
    of interest is set as the argument. The response is set by
    getting the information via an HTTPS request of the url + category,
    and then a Beautiful Soup object is created. This soup then selects
    the class .news-card, establishes an empty list for the articles, and
    then incorporates a for-loop strategy to add the parsed card per category
    to the list for each card therein. At last, the articles are returned. 
    '''
    url = 'https://inshorts.com/en/read/' + category
    response = get(url)
    soup = BeautifulSoup(response.text)

    cards = soup.select('.news-card')
    articles = []

    for card in cards:
        articles.append(parse_news_card(card, category))

    return articles

def get_news_articles(use_cache=True):
    '''
    Using a caching approach, this function first checks to determine whether
    a .json concerning this target is extant. If so, it immediately returns a data
    frame containing the identified categories by using Pandas to read that .json. Extend
    is used in favor of append, as every dictionary in the list of dictionary needs to be
    continually added to the articles list, which is then converted into a dataframe.
    Lastly, a .json is created so that future uses will automatically read via the cache,
    as long as it abides by the naming convention set here.
    '''
    
    if os.path.exists('news_articles.json') and use_cache:
        return pd.read_json('news_articles.json')

    categories = ['business', 'sports', 'technology', 'entertainment']

    articles = []

    for category in categories:
        print(f'Getting {category} articles')
        articles.extend(parse_news_category(category))

    df = pd.DataFrame(articles)
    df.to_json('news_articles.json', orient='records')
    return df

