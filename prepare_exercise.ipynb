{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16bd3d5f-72a2-4889-b47b-7ff49c3b8c1f",
   "metadata": {},
   "source": [
    "# Parsing Text     \n",
    "Taking acquired data and breaking it down into smaller components; analyzing a sentence by its parts and then describing the syntactic roles among those elements. \n",
    "\n",
    "Planning and Sequence: \n",
    "\n",
    "1. Convert text to all lower-case for normalcy and consistency.\n",
    "2. Remove any accented characters, non-ASCII (American Standard Code for Information Interchange) characters.\n",
    "3. Remove special characters. (clarification listed below) \n",
    "4. Stem or lemmatize the words. (notes below)\n",
    "5. Remove stopwords. (notes below)\n",
    "6. Store the clean text and the original text for use in future notebooks.\n",
    "\n",
    "+ \"Special characters include all printable characters that are neither letters nor numbers. These include punctuation or technical, mathematical characters. ASCII also includes the space (a non-visible but printable character), and therefore, does not belong to the control characters category, as one might suspect.\" - [source](https://www.ionos.com/digitalguide/server/know-how/ascii-codes-overview-of-all-characters-on-the-ascii-table/)\n",
    "\n",
    "+ \"Stemming and Lemmatization both generate the foundation sort of the inflected words and therefore the only difference is that stem may not be an actual word whereas, lemma is an actual language word. Stemming follows an algorithm with steps to perform on the words which makes it faster.\" - [source](https://towardsdatascience.com/stemming-vs-lemmatization-2daddabcb221)\n",
    "\n",
    "+ Stop-words are words which don't have strong, meaningful connotations; for instance, ‘and’, ‘a’, ‘it's’, ‘they’. Articles, prepositions, pronouns, conjunctions, and the like.  Although useful in natural language, computational inference from these words is weak; at least for the time being. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64fdb697-d24e-43e3-a05f-a8a43473ce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80466bdb-9e5c-49cc-9722-2dccb704be3b",
   "metadata": {},
   "source": [
    "***\n",
    "## Exercises\n",
    "\n",
    "The end result of this exercise should be a file named prepare.py that defines the requested functions.\n",
    "\n",
    "In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired.\n",
    "\n",
    "1. Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "- Lowercase everything\n",
    "- Normalize unicode characters\n",
    "- Replace anything that is not a letter, number, whitespace or a single quote.\n",
    "\n",
    "2. Define a function named tokenize. It should take in a string and tokenize all the words in the string.\n",
    "\n",
    "3. Define a function named stem. It should accept some text and return the text after applying stemming to all the words.\n",
    "\n",
    "4. Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word.\n",
    "\n",
    "5. Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords.\n",
    "\n",
    "- This function should define two optional parameters, extra_words and exclude_words. \n",
    "    - These parameters should define any additional stop words to include, and any words that we don't want to remove.\n",
    "\n",
    "6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df.\n",
    "\n",
    "7. Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df.\n",
    "\n",
    "8. For each dataframe, produce the following columns:\n",
    "\n",
    "- title to hold the title\n",
    "- original to hold the original article/post content\n",
    "- clean to hold the normalized and tokenized original with the stopwords removed.\n",
    "- stemmed to hold the stemmed version of the cleaned data.\n",
    "- lemmatized to hold the lemmatized version of the cleaned data.\n",
    "\n",
    "9. Ask yourself:\n",
    "\n",
    "- If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "- If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "- If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561e47e8-89b6-46ae-82e1-465b646dff84",
   "metadata": {},
   "source": [
    "#### 1. Define a function named basic_clean.      \n",
    "It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "- Lowercase everything\n",
    "- Normalize unicode characters\n",
    "- Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fd1ce0-ced2-4344-9be6-e2f8667108e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6a1a352-e053-4740-acc9-67b2345d4dbc",
   "metadata": {},
   "source": [
    "*** \n",
    "#### 2. Define a function named tokenize.      \n",
    "It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe1d095-fb7f-46aa-9b97-669f958c2f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "764945aa-c1e4-4b39-9e71-a22e7212a4ab",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### 3. Define a function named stem.      \n",
    "It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1e767f-19c1-43b0-af25-f6223518d2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4db1c69-c5ae-4c1d-9118-1967af851799",
   "metadata": {},
   "source": [
    "***\n",
    "#### 4. Define a function named lemmatize.       \n",
    "It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d9cacc-fe5f-4478-86f2-c0401e26928b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ee3dcab-9835-4963-a167-6a69622b5e17",
   "metadata": {},
   "source": [
    "*** \n",
    "#### 5. Define a function named remove_stopwords.      \n",
    "It should accept some text and return the text after removing all the stopwords.\n",
    "\n",
    "- This function should define two optional parameters, extra_words and exclude_words. \n",
    "    - These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c29533-2bf5-42d9-98a7-f43b96d113f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e107698-5d5b-4ee4-a0d3-1afda688618a",
   "metadata": {},
   "source": [
    "***\n",
    "#### 6. Use your data from the acquire to produce a dataframe of the news articles.      \n",
    "Name the dataframe news_df.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62725427-20e6-400a-82c8-bcd85a72ace3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0574a3e3-d057-4cd1-ba81-4b6a50130500",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### 7. Make another dataframe for the Codeup blog posts.      \n",
    "Name the dataframe codeup_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea238c9c-17d1-496d-be07-b39ffe64a91c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "183979a4-3648-4a21-bd5f-61109f23d6ba",
   "metadata": {},
   "source": [
    "***\n",
    "#### 8. For each dataframe, produce the following columns:\n",
    "\n",
    "- title to hold the title\n",
    "- original to hold the original article/post content\n",
    "- clean to hold the normalized and tokenized original with the stopwords removed.\n",
    "- stemmed to hold the stemmed version of the cleaned data.\n",
    "- lemmatized to hold the lemmatized version of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d904127-7202-48d0-b284-7f9f8a88445c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0d6ddc6-8f36-46f5-8aaa-5e922220187a",
   "metadata": {},
   "source": [
    "***\n",
    "#### 9. Ask yourself:\n",
    "\n",
    "- If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "- If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "- If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e540ec4-fe70-497d-99be-00a8ed455895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65135b80-9ad1-471b-b7ba-6c7194bf3369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b0838-f9e2-4483-bc96-57901e78db70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
